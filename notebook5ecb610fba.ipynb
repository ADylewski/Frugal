{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-05T09:04:29.523439Z",
     "iopub.status.busy": "2026-01-05T09:04:29.523163Z",
     "iopub.status.idle": "2026-01-05T09:04:38.606393Z",
     "shell.execute_reply": "2026-01-05T09:04:38.604572Z",
     "shell.execute_reply.started": "2026-01-05T09:04:29.523421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) Install deps (Kaggle / Colab)\n",
    "# =========================\n",
    "# If you're on Kaggle, you can run this in a notebook cell.\n",
    "!pip -q install opencv-python-headless scikit-learn mlxtend tqdm scikit-image kagglehub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T09:04:38.608509Z",
     "iopub.status.busy": "2026-01-05T09:04:38.608202Z",
     "iopub.status.idle": "2026-01-05T09:04:38.631475Z",
     "shell.execute_reply": "2026-01-05T09:04:38.630094Z",
     "shell.execute_reply.started": "2026-01-05T09:04:38.608479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylesm/Frugal/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Imports + config\n",
    "# =========================\n",
    "import os, glob, random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.648028Z",
     "iopub.status.idle": "2026-01-05T09:02:39.648302Z",
     "shell.execute_reply": "2026-01-05T09:02:39.648155Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.648145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/dylesm/.cache/kagglehub/datasets/vesuvius13/formula-one-cars/versions/1\n",
      "Found images: 2409\n",
      "Example: ['/Users/dylesm/.cache/kagglehub/datasets/vesuvius13/formula-one-cars/versions/1/Formula One Cars/Williams F1 car/00000400.jpg', '/Users/dylesm/.cache/kagglehub/datasets/vesuvius13/formula-one-cars/versions/1/Formula One Cars/Williams F1 car/00000372.jpg', '/Users/dylesm/.cache/kagglehub/datasets/vesuvius13/formula-one-cars/versions/1/Formula One Cars/Williams F1 car/00000414.jpg']\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Point this at your Kaggle dataset images\n",
    "# =========================\n",
    "# Change this to your dataset root.\n",
    "\n",
    "\n",
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"vesuvius13/formula-one-cars\")\n",
    "path = \"/Users/dylesm/.cache/kagglehub/datasets/vesuvius13/formula-one-cars/versions/1\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATA_ROOT = path\n",
    "\n",
    "# Common image extensions\n",
    "EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n",
    "\n",
    "def find_images(root):\n",
    "    paths = []\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for fn in filenames:\n",
    "            if fn.lower().endswith(EXTS):\n",
    "                paths.append(os.path.join(dirpath, fn))\n",
    "    return paths\n",
    "\n",
    "image_paths = find_images(DATA_ROOT)\n",
    "print(\"Found images:\", len(image_paths))\n",
    "print(\"Example:\", image_paths[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.649359Z",
     "iopub.status.idle": "2026-01-05T09:02:39.649597Z",
     "shell.execute_reply": "2026-01-05T09:02:39.649506Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.649498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 87/300 [00:01<00:03, 54.33it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|██████████| 300/300 [00:05<00:00, 52.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) Load + normalize images (resize)\n",
    "# =========================\n",
    "IMG_SIZE = (256, 256)   # Keep it manageable to start\n",
    "MAX_IMAGES = 300        # Start small; scale later\n",
    "\n",
    "random.shuffle(image_paths)\n",
    "image_paths_small = image_paths[:min(MAX_IMAGES, len(image_paths))]\n",
    "\n",
    "def load_image_bgr(path, size=IMG_SIZE):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "imgs = []\n",
    "kept_paths = []\n",
    "for p in tqdm(image_paths_small):\n",
    "    im = load_image_bgr(p)\n",
    "    if im is not None:\n",
    "        imgs.append(im)\n",
    "        kept_paths.append(p)\n",
    "\n",
    "print(\"Loaded:\", len(imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.651033Z",
     "iopub.status.idle": "2026-01-05T09:02:39.651526Z",
     "shell.execute_reply": "2026-01-05T09:02:39.651358Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.651339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches: 64 Grid: (8, 8)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Split into patches (per image)\n",
    "# =========================\n",
    "PATCH = 32  # 32x32 patches\n",
    "# With IMG_SIZE=256 and PATCH=32 => 8x8 grid\n",
    "\n",
    "def image_to_patches(img_bgr, patch=PATCH):\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    assert h % patch == 0 and w % patch == 0\n",
    "    patches = []\n",
    "    coords = []\n",
    "    for y in range(0, h, patch):\n",
    "        for x in range(0, w, patch):\n",
    "            patches.append(img_bgr[y:y+patch, x:x+patch])\n",
    "            coords.append((y // patch, x // patch))  # (row, col) in patch grid\n",
    "    return patches, coords, (h // patch, w // patch)\n",
    "\n",
    "# quick test\n",
    "patches0, coords0, grid0 = image_to_patches(imgs[0])\n",
    "print(\"Patches:\", len(patches0), \"Grid:\", grid0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.653162Z",
     "iopub.status.idle": "2026-01-05T09:02:39.653485Z",
     "shell.execute_reply": "2026-01-05T09:02:39.653370Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.653356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dim: (522,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Patch feature extractor (simple + robust)\n",
    "#    - Color histogram in HSV\n",
    "#    - Texture via LBP histogram\n",
    "# =========================\n",
    "LBP_P = 8\n",
    "LBP_R = 1\n",
    "LBP_METHOD = \"uniform\"\n",
    "LBP_BINS = LBP_P + 2  # uniform LBP bins\n",
    "\n",
    "def hsv_hist(patch_bgr, h_bins=8, s_bins=8, v_bins=8):\n",
    "    hsv = cv2.cvtColor(patch_bgr, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0,1,2], None, [h_bins,s_bins,v_bins], [0,180, 0,256, 0,256])\n",
    "    hist = hist.flatten().astype(np.float32)\n",
    "    hist /= (hist.sum() + 1e-8)\n",
    "    return hist\n",
    "\n",
    "def lbp_hist(patch_bgr):\n",
    "    gray = cv2.cvtColor(patch_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=LBP_P, R=LBP_R, method=LBP_METHOD)\n",
    "    # histogram over LBP codes\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, LBP_BINS+1), range=(0, LBP_BINS))\n",
    "    hist = hist.astype(np.float32)\n",
    "    hist /= (hist.sum() + 1e-8)\n",
    "    return hist\n",
    "\n",
    "def patch_features(patch_bgr):\n",
    "    return np.concatenate([hsv_hist(patch_bgr), lbp_hist(patch_bgr)], axis=0)\n",
    "\n",
    "# feature length check\n",
    "feat0 = patch_features(patches0[0])\n",
    "print(\"Feature dim:\", feat0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.653986Z",
     "iopub.status.idle": "2026-01-05T09:02:39.654215Z",
     "shell.execute_reply": "2026-01-05T09:02:39.654110Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.654100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:01<00:00, 171.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patch samples: (19200, 522)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Build a big feature matrix from many patches (for clustering)\n",
    "# =========================\n",
    "# To keep memory sane, subsample patches per image at first.\n",
    "PATCHES_PER_IMAGE = 64  # take all if image is 8x8=64 patches; otherwise sample\n",
    "\n",
    "all_feats = []\n",
    "all_meta = []  # (img_idx, patch_row, patch_col) for later mapping\n",
    "\n",
    "for i, img in enumerate(tqdm(imgs)):\n",
    "    patches, coords, (gh, gw) = image_to_patches(img)\n",
    "    idxs = list(range(len(patches)))\n",
    "    if len(idxs) > PATCHES_PER_IMAGE:\n",
    "        idxs = random.sample(idxs, PATCHES_PER_IMAGE)\n",
    "\n",
    "    for j in idxs:\n",
    "        all_feats.append(patch_features(patches[j]))\n",
    "        pr, pc = coords[j]\n",
    "        all_meta.append((i, pr, pc))\n",
    "\n",
    "X = np.vstack(all_feats)\n",
    "print(\"Total patch samples:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.655257Z",
     "iopub.status.idle": "2026-01-05T09:02:39.655519Z",
     "shell.execute_reply": "2026-01-05T09:02:39.655390Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.655379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Quantize patches into \"visual words\" with k-means\n",
    "# =========================\n",
    "K = 200  # vocabulary size (start 100-300)\n",
    "kmeans = MiniBatchKMeans(n_clusters=K, batch_size=4096, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "print(\"Trained kmeans with K =\", K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kmeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m word_grids = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(imgs):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     word_grids.append(\u001b[43mimage_words_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExample grid shape:\u001b[39m\u001b[33m\"\u001b[39m, word_grids[\u001b[32m0\u001b[39m].shape)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExample grid IDs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, word_grids[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mimage_words_grid\u001b[39m\u001b[34m(img_bgr, patch)\u001b[39m\n\u001b[32m      5\u001b[39m patches, coords, (gh, gw) = image_to_patches(img_bgr, patch=patch)\n\u001b[32m      6\u001b[39m feats = np.vstack([patch_features(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patches])\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m words = \u001b[43mkmeans\u001b[49m.predict(feats)  \u001b[38;5;66;03m# [num_patches]\u001b[39;00m\n\u001b[32m      8\u001b[39m grid = words.reshape(gh, gw)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grid\n",
      "\u001b[31mNameError\u001b[39m: name 'kmeans' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 8) Assign every patch in every image to a visual word\n",
    "# =========================\n",
    "def image_words_grid(img_bgr, patch=PATCH):\n",
    "    patches, coords, (gh, gw) = image_to_patches(img_bgr, patch=patch)\n",
    "    feats = np.vstack([patch_features(p) for p in patches])\n",
    "    words = kmeans.predict(feats)  # [num_patches]\n",
    "    grid = words.reshape(gh, gw)\n",
    "    return grid  # shape: (gh, gw)\n",
    "\n",
    "word_grids = []\n",
    "for img in tqdm(imgs):\n",
    "    word_grids.append(image_words_grid(img))\n",
    "\n",
    "print(\"Example grid shape:\", word_grids[0].shape)\n",
    "print(\"Example grid IDs:\\n\", word_grids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.656933Z",
     "iopub.status.idle": "2026-01-05T09:02:39.657381Z",
     "shell.execute_reply": "2026-01-05T09:02:39.657245Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.657227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def assign_words(X, centers):\n",
    "    x2 = (X**2).sum(axis=1, keepdims=True)\n",
    "    c2 = (centers**2).sum(axis=1)[None, :]\n",
    "    dist = x2 + c2 - 2.0 * (X @ centers.T)\n",
    "    return dist.argmin(axis=1).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.658788Z",
     "iopub.status.idle": "2026-01-05T09:02:39.659117Z",
     "shell.execute_reply": "2026-01-05T09:02:39.658983Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.658972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_grids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tx\n\u001b[32m     22\u001b[39m transactions = []\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m grid \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mword_grids\u001b[49m):\n\u001b[32m     24\u001b[39m     transactions.extend(local_transactions_from_grid(grid))\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal transactions:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(transactions))\n",
      "\u001b[31mNameError\u001b[39m: name 'word_grids' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 9) Build LOCAL transactions (3x3 neighborhoods)\n",
    "#    Each transaction is a set of word IDs co-occurring locally.\n",
    "# =========================\n",
    "NEIGH = 1  # 1 => 3x3 window; 2 => 5x5\n",
    "\n",
    "def local_transactions_from_grid(grid, neigh=NEIGH):\n",
    "    gh, gw = grid.shape\n",
    "    tx = []\n",
    "    for r in range(gh):\n",
    "        for c in range(gw):\n",
    "            r0, r1 = max(0, r-neigh), min(gh, r+neigh+1)\n",
    "            c0, c1 = max(0, c-neigh), min(gw, c+neigh+1)\n",
    "            window = grid[r0:r1, c0:c1].ravel()\n",
    "            # Use unique items so transaction is a set\n",
    "            items = list(set(int(x) for x in window))\n",
    "            # prefix items so TransactionEncoder treats them as categorical tokens\n",
    "            items = [f\"w{it}\" for it in items]\n",
    "            tx.append(items)\n",
    "    return tx\n",
    "\n",
    "transactions = []\n",
    "for grid in tqdm(word_grids):\n",
    "    transactions.extend(local_transactions_from_grid(grid))\n",
    "\n",
    "print(\"Total transactions:\", len(transactions))\n",
    "print(\"Example transaction:\", transactions[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.659695Z",
     "iopub.status.idle": "2026-01-05T09:02:39.659896Z",
     "shell.execute_reply": "2026-01-05T09:02:39.659812Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.659803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mine_pair_rules_from_sets(transactions):\n",
    "    item_count = Counter()\n",
    "    pair_count = Counter()\n",
    "    n_tx = len(transactions)\n",
    "\n",
    "    for s in transactions:\n",
    "        items = sorted(s)\n",
    "        for a in items:\n",
    "            item_count[a] += 1\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i+1, len(items)):\n",
    "                pair_count[(items[i], items[j])] += 1\n",
    "\n",
    "    rules = []\n",
    "    for (a, b), pab in pair_count.items():\n",
    "        sup = pab / n_tx\n",
    "        conf_ab = pab / item_count[a]\n",
    "        conf_ba = pab / item_count[b]\n",
    "        rules.append((a, b, sup, conf_ab, conf_ba))\n",
    "\n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.660724Z",
     "iopub.status.idle": "2026-01-05T09:02:39.660911Z",
     "shell.execute_reply": "2026-01-05T09:02:39.660830Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.660822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MIN_SUPPORT = 0.005\n",
    "MIN_CONF = 0.25\n",
    "\n",
    "sym_pairs = []\n",
    "for a, b, sup, conf_ab, conf_ba in mine_pair_rules_from_sets(transactions):\n",
    "    if sup >= MIN_SUPPORT and conf_ab >= MIN_CONF and conf_ba >= MIN_CONF:\n",
    "        avg_conf = 0.5 * (conf_ab + conf_ba)\n",
    "        sym_pairs.append((avg_conf, sup, a, b))\n",
    "\n",
    "sym_pairs.sort(reverse=True)\n",
    "print(\"Top symmetric pairs:\", sym_pairs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.662009Z",
     "iopub.status.idle": "2026-01-05T09:02:39.662288Z",
     "shell.execute_reply": "2026-01-05T09:02:39.662140Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.662130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_single_merge(transactions, a, b, new_id):\n",
    "    out = []\n",
    "    for s in transactions:\n",
    "        s2 = set(s)\n",
    "        if a in s2 and b in s2:\n",
    "            s2.remove(a)\n",
    "            s2.remove(b)\n",
    "            s2.add(new_id)\n",
    "        out.append(s2)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-05T09:02:39.662899Z",
     "iopub.status.idle": "2026-01-05T09:02:39.663151Z",
     "shell.execute_reply": "2026-01-05T09:02:39.663036Z",
     "shell.execute_reply.started": "2026-01-05T09:02:39.663026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 13) (Next step) Wrap merging into an iterative loop (skeleton)\n",
    "#     This is where you recreate the full paper-style iterative merging.\n",
    "# =========================\n",
    "def mine_pair_rules(transactions, min_support=0.005, min_conf=0.25):\n",
    "    import pandas as pd\n",
    "    te = TransactionEncoder()\n",
    "    T = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(T, columns=te.columns_)\n",
    "    freq = apriori(df, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(freq, metric=\"confidence\", min_threshold=min_conf)\n",
    "    pair_rules = rules[\n",
    "        rules[\"antecedents\"].apply(is_singleton) &\n",
    "        rules[\"consequents\"].apply(is_singleton)\n",
    "    ].copy()\n",
    "    return pair_rules\n",
    "\n",
    "def best_symmetric_pair(pair_rules):\n",
    "    conf_map = {}\n",
    "    for _, row in pair_rules.iterrows():\n",
    "        a = next(iter(row[\"antecedents\"]))\n",
    "        b = next(iter(row[\"consequents\"]))\n",
    "        conf_map[(a, b)] = float(row[\"confidence\"])\n",
    "    best = None\n",
    "    for (a, b), cab in conf_map.items():\n",
    "        cba = conf_map.get((b, a))\n",
    "        if cba is None:\n",
    "            continue\n",
    "        avg = 0.5 * (cab + cba)\n",
    "        if (best is None) or (avg > best[0]):\n",
    "            best = (avg, a, b)\n",
    "    return best  # (avg_conf, a, b) or None\n",
    "\n",
    "def iterative_merging(transactions, iters=5, min_support=0.005, min_conf=0.25):\n",
    "    tx = transactions\n",
    "    merges = []\n",
    "    for t in range(iters):\n",
    "        pair_rules = mine_pair_rules(tx, min_support=min_support, min_conf=min_conf)\n",
    "        best = best_symmetric_pair(pair_rules)\n",
    "        if best is None:\n",
    "            print(f\"Stop: no symmetric pair at iter {t}.\")\n",
    "            break\n",
    "        avg_conf, a, b = best\n",
    "        new_token = f\"p{t}({a}+{b})\"\n",
    "        tx = apply_single_merge(tx, a, b, new_token)\n",
    "        merges.append((new_token, a, b, avg_conf))\n",
    "        print(f\"Iter {t}: merge {a}<->{b} avg_conf={avg_conf:.3f} => {new_token}\")\n",
    "    return tx, merges\n",
    "\n",
    "# Run a tiny number of merges to start\n",
    "tx_final, merges = iterative_merging(transactions, iters=3, min_support=MIN_SUPPORT, min_conf=MIN_CONF)\n",
    "print(\"Merges:\", merges)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1867053,
     "sourceId": 3049087,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
